# SQL Accuracy Evaluation System Documentation

## Overview
The SQL Accuracy Evaluation system assesses the quality and correctness of SQL queries generated by the NL2SQL agent. It provides detailed analysis of SQL syntax, structure, and semantic accuracy compared to reference queries.

## Core Components

### 1. SQLAccuracyEvaluator Class
**Location:** `eval/sql_accuracy_evaluator.py`

The main evaluator that analyzes SQL query quality:

#### Key Functions:

**`__init__(config: SQLAccuracyConfig)`**
- Initializes evaluator with project configuration
- Sets up evaluation parameters and thresholds
- Prepares SQL parsing and analysis tools

**`extract_sql_from_response(response: str)`**
- Extracts SQL queries from agent responses
- Handles multiple response formats (JSON, code blocks, plain text)
- Uses pattern matching to identify SQL content
- Returns cleaned SQL query or None

**`extract_components(sql: str)`**
- Parses SQL query into structured components
- Extracts SELECT, FROM, WHERE, GROUP BY, ORDER BY, LIMIT clauses
- Identifies aggregations and table references
- Returns structured SQL analysis

**`sql_comparison(agent_sql: str, reference_sql: str)`**
- Compares agent-generated SQL against reference SQL
- Analyzes syntax similarity and semantic correctness
- Calculates accuracy scores across multiple dimensions
- Returns comprehensive comparison results

### 2. SQL Component Extraction Functions

**`extract_select_columns(select_token)`**
- Extracts column names from SELECT clause
- Handles aliases and expressions
- Returns list of selected columns

**`extract_from_tables(from_token)`**
- Extracts table names from FROM clause
- Handles joins and subqueries
- Returns list of referenced tables

**`extract_where_conditions(where_token)`**
- Extracts WHERE clause conditions
- Parses comparison operators and logical operators
- Returns structured condition analysis

**`extract_group_by(group_token)`**
- Extracts GROUP BY clause columns
- Handles multiple grouping levels
- Returns grouping column list

**`extract_order_by(order_token)`**
- Extracts ORDER BY clause specifications
- Handles ASC/DESC modifiers
- Returns ordering specifications

**`extract_aggregations(select_columns: list)`**
- Identifies aggregation functions (SUM, COUNT, AVG, etc.)
- Extracts aggregation column names
- Returns list of aggregations found

## SQL Extraction Process

### 1. Response Format Detection
```python
# Try JSON format first
try:
    response_data = json.loads(response)
    if 'sql' in response_data:
        return response_data['sql']
except (json.JSONDecodeError, KeyError):
    pass

# Try code block format
sql_pattern = r"```sql\s*(.*?)\s*```"
match = re.search(sql_pattern, response, re.DOTALL | re.IGNORECASE)
if match:
    return match.group(1).strip()

# Try plain text with SQL keywords
sql_keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT']
if any(keyword in response.upper() for keyword in sql_keywords):
    # Extract SQL lines
```

**What happens:**
- Attempts JSON parsing for structured responses
- Searches for SQL code blocks with markdown formatting
- Falls back to keyword-based extraction
- Returns cleaned SQL query

### 2. SQL Component Analysis
```python
# Parse SQL into components
parsed = sqlparse.parse(sql)[0]
components = {
    'select': [],
    'from': [],
    'where': [],
    'group_by': [],
    'order_by': [],
    'limit': None,
    'aggregations': []
}

# Extract each clause
for token in parsed.tokens:
    if hasattr(token, 'value'):
        if token.get_name() == 'SELECT':
            components['select'] = self.extract_select_columns(token)
        elif token.get_name() == 'FROM':
            components['from'] = self.extract_from_tables(token)
        # ... other clauses
```

**What happens:**
- Uses sqlparse library for SQL parsing
- Extracts each major SQL clause
- Identifies table references and column names
- Detects aggregation functions

### 3. SQL Comparison Analysis
```python
# Compare agent SQL vs reference SQL
def sql_comparison(self, agent_sql: str, reference_sql: str) -> Dict[str, Any]:
    agent_components = self.extract_components(agent_sql)
    reference_components = self.extract_components(reference_sql)
    
    comparison = {
        'syntax_similarity': self._calculate_syntax_similarity(agent_components, reference_components),
        'table_usage_correct': self._compare_tables(agent_components['from'], reference_components['from']),
        'column_selection_correct': self._compare_columns(agent_components['select'], reference_components['select']),
        'where_conditions_correct': self._compare_conditions(agent_components['where'], reference_components['where']),
        'aggregation_correct': self._compare_aggregations(agent_components['aggregations'], reference_components['aggregations'])
    }
    
    return comparison
```

**What happens:**
- Extracts components from both SQL queries
- Compares syntax structure and complexity
- Evaluates table usage accuracy
- Assesses column selection correctness
- Analyzes WHERE condition accuracy
- Checks aggregation function usage

## Evaluation Metrics

### 1. Syntax Similarity
- **Metric:** Percentage similarity (0.0 - 1.0)
- **Evaluation:** Compares overall SQL structure
- **Factors:** Clause presence, keyword usage, query complexity

### 2. Table Usage Accuracy
- **Metric:** Binary score (1.0 = correct, 0.0 = incorrect)
- **Evaluation:** Compares actual vs expected table references
- **Detection:** FROM clause analysis

### 3. Column Selection Accuracy
- **Metric:** Percentage of correct columns (0.0 - 1.0)
- **Evaluation:** Compares SELECT clause columns
- **Factors:** Required columns, optional columns, aliases

### 4. WHERE Condition Accuracy
- **Metric:** Binary score (1.0 = correct, 0.0 = incorrect)
- **Evaluation:** Compares WHERE clause conditions
- **Factors:** Filter conditions, logical operators, comparison operators

### 5. Aggregation Accuracy
- **Metric:** Binary score (1.0 = correct, 0.0 = incorrect)
- **Evaluation:** Compares aggregation functions
- **Detection:** SUM, COUNT, AVG, MAX, MIN functions

## SQL Pattern Detection

### 1. SQL Keyword Detection
```python
sql_keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'LIMIT', 'LEFT JOIN', 'SUM']
if any(keyword in response.upper() for keyword in sql_keywords):
    # SQL content detected
```

**What happens:**
- Scans response for SQL keywords
- Uses case-insensitive matching
- Identifies SQL content boundaries

### 2. Table Name Extraction
```python
table_pattern = [
    r'FROM\s+([a-zA-Z0-9_\.]+)'  # Matches "FROM table_name"
]
for pattern in table_pattern:
    match = re.search(pattern, response, re.IGNORECASE)
    if match:
        return match.group(1)
```

**What happens:**
- Uses regex patterns to find table names
- Supports qualified table names (database.table)
- Handles multiple table references

### 3. Aggregation Function Detection
```python
aggregation_functions = ['SUM', 'COUNT', 'AVG', 'MAX', 'MIN', 'DISTINCT']
for func in aggregation_functions:
    if func in select_text.upper():
        aggregations.append(func)
```

**What happens:**
- Scans SELECT clause for aggregation functions
- Identifies function names and arguments
- Tracks aggregation complexity

## Integration Points

### 1. Database Agent Evaluation
**Location:** `data_science/tools.py:29` - `call_db_agent()`

```python
# SQL evaluation can be triggered after database agent execution
if db_agent_output:
    # Extract SQL from response
    sql_evaluator = SQLAccuracyEvaluator(config)
    extracted_sql = sql_evaluator.extract_sql_from_response(str(db_agent_output))
    
    if extracted_sql:
        # Compare with reference SQL if available
        comparison = sql_evaluator.sql_comparison(extracted_sql, reference_sql)
```

**What happens:**
- Evaluates SQL after database agent execution
- Extracts SQL from agent response
- Compares against reference queries
- Logs accuracy metrics

### 2. Trajectory Evaluation Integration
**Location:** `data_science/utils/trajectory_evaluator.py`

```python
# SQL accuracy can be included in trajectory evaluation
if actual_trajectory['sql_generation']:
    sql_evaluator = SQLAccuracyEvaluator(config)
    sql_accuracy = sql_evaluator.evaluate_sql_accuracy(
        agent_sql=extracted_sql,
        reference_sql=test_case.get('reference_sql')
    )
```

**What happens:**
- Integrates SQL accuracy into trajectory evaluation
- Provides SQL-specific metrics
- Enhances overall evaluation coverage

## BigQuery Integration

### SQL Evaluations Table
- `evaluation_id`: Primary key
- `session_id`: Links to session
- `agent_sql`: Generated SQL query
- `reference_sql`: Expected SQL query
- `syntax_similarity`: Syntax comparison score
- `table_usage_correct`: Table accuracy boolean
- `column_selection_correct`: Column accuracy boolean
- `where_conditions_correct`: WHERE clause accuracy boolean
- `aggregation_correct`: Aggregation accuracy boolean
- `overall_accuracy`: Combined accuracy score
- `evaluation_timestamp`: When evaluation ran

## Error Handling

The SQL evaluator implements robust error handling:
- **Parsing Errors:** Graceful handling of malformed SQL
- **Missing Components:** Default values for missing clauses
- **Comparison Errors:** Fallback mechanisms for comparison failures
- **Extraction Errors:** Multiple extraction strategies with fallbacks

## Performance Considerations

- **Lazy Parsing:** SQL parsing only when needed
- **Caching:** Component extraction results cached
- **Efficient Comparison:** Optimized comparison algorithms
- **Memory Management:** Minimal data retention

## Debugging and Monitoring

The SQL evaluator provides comprehensive debugging:
- SQL extraction confirmations
- Component analysis details
- Comparison breakdowns
- Error details with context
- Performance metrics

This SQL accuracy evaluation system ensures high-quality SQL generation and provides detailed metrics for NL2SQL agent improvement. 